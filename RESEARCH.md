# Roadmap for the Irish Morphology Model (UD Treebank-Based)

## **Output Scope: Token-Level Tags vs. Full UD Annotations**

**Token-Level UD Tags (POS \+ Morphological Features):** Focusing the model on predicting Universal POS tags and UD morphological features is the most straightforward approach. The Irish UD treebank provides gold UPOS and rich morphological FEATS for each token[\[1\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Includes%20text%3A%20yes%20Lemmas%3A%20manual,SA%203.0). These features capture Irish-specific morphology (e.g. *initial mutations, inflectional forms*) using both standard UD categories and some new Irish-specific labels[\[2\]](https://ceur-ws.org/Vol-1779/09lynn.pdf#:~:text=in%02ventory%20of%20the%20UD%20Project,specific%20labels). A model trained to predict UPOS+FEATS for each token can leverage this data to achieve high accuracy in morphological tagging. In fact, state-of-the-art pipelines already reach \~95% accuracy on combined POS+feature tags for Irish[\[3\]](https://stanfordnlp.github.io/stanza/performance.html#:~:text=IrishIDTga%2099,16). Focusing on token-level tags keeps the model self-contained and avoids external dependencies. It directly benefits morphologically-driven tools (e.g. conjugation or declension modules) by providing the grammatical features for each word in context.

**Including Lemmas:** It is feasible to extend the model to predict lemmas for each token, which is often considered part of morphological analysis. The UD treebank includes manually curated lemmas for Irish[\[1\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Includes%20text%3A%20yes%20Lemmas%3A%20manual,SA%203.0), so a data-driven lemmatizer can be trained alongside tagging. Incorporating lemma prediction would allow the model to output the base form of each word – useful for downstream generation or dictionary lookup. However, lemmatization is a sequence prediction task (mapping an inflected word to its canonical form) and typically slightly harder than tagging; for Irish, even the best UD pipeline only gets \~89% lemma accuracy[\[3\]](https://stanfordnlp.github.io/stanza/performance.html#:~:text=IrishIDTga%2099,16). Still, this is **possible without major external resources**: we can train a small character-based seq2seq decoder or a classification model that outputs the lemma string for each token using the UD training pairs. The model can share the same encoder for context, ensuring it chooses the correct lemma when surface forms are ambiguous due to mutations or inflections. In summary, adding lemma output is achievable with the UD data and would greatly enhance the morphological tool, so it’s recommended as part of the model’s outputs (with the understanding that accuracy might be a bit lower than POS tags).

**Dependency Heads/Labels (Full Parsing) and MWE:** Incorporating full dependency parsing (i.e. predicting each token’s head index and dependency label) and multi-word expression (MWE) identification would significantly increase model complexity. The UD treebank does provide dependency heads/relations and marks MWEs through specific relations (e.g. fixed for compound prepositions, flat for names)[\[4\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Note%3A%20,word%20units%20%28fixed). In theory, a single model could be multi-tasked to perform parsing in addition to tagging. However, training an accurate dependency parser on \~4,000 sentences is challenging – current neural parsers (without large pretraining) achieve around 80–82% LAS on Irish[\[3\]](https://stanfordnlp.github.io/stanza/performance.html#:~:text=IrishIDTga%2099,16), leaving nearly 20% of relations wrong. Including these outputs would require designing a parsing head (e.g. a biaffine scorer for head selection) on top of the encoder, and carefully tuning a multi-task loss. This is **possible** (many UD tools like UDPipe/Stanza handle parsing and tagging together), but it’s a **much larger effort** than tagging alone. It also introduces potential error propagation into the morphology tools (e.g. if parse is wrong, MWE identification via the fixed/flat labels would be wrong). Given that our primary goal is a **morphological analyzer** for Irish, it’s advisable to **defer full dependency parsing and MWE handling** for now. These can be added in a later phase or handled by a separate module. In short, the model should initially output **just token-level UD labels (POS \+ FEATS, and lemmas)** – this is where we have solid data and can train effectively without heavy external development. Dependency parse output is a nice-to-have for syntax-related applications, but not essential for the core morph tools and would entail substantial extra work (training and maintaining a parsing component outside the current scope).  
\- *Gap Note:* Full parsing would require a dedicated parser architecture and yields moderate accuracy (\~81% LAS)[\[3\]](https://stanfordnlp.github.io/stanza/performance.html#:~:text=IrishIDTga%2099,16). If needed later, one could train a second-stage parser that takes the tagged sentence and produces dependency relations. MWEs in Irish UD are annotated via dependency relations (fixed, flat, etc.[\[4\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Note%3A%20,word%20units%20%28fixed)), so an accurate parser would be needed to recover them. This gap can be filled in future work without blocking the morphology model now.

## **Inference Design: Standalone Model vs. Analyzer-Assisted**

**Model Inputs at Inference:** We assume the ONNX model will process one sentence at a time, with the text already tokenized into words. This is reasonable and keeps the interface simple – the Rust pipeline can call the tokenizer (e.g. a Finite State tokenizer as used in the UD corpus[\[5\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=The%20tokenisation%20of%20the%20Irish,2002)) beforehand, then feed the token list into the model. The ONNX model’s input will thus be a sequence of token identifiers (or subword/character encodings) representing a sentence. No other contextual information (like analyzer hints) will be required. Keeping the input as plain tokenized text ensures the model is self-contained and can be easily called from Rust with minimal pre-processing.

**Excluding BuNaMo/Gramadán Suggestions:** In the current design, the model does **not rely on external rule-based analyzer outputs at runtime**. That is, we train it to infer the correct tags from context and word form alone, rather than expecting a list of candidate analyses from BuNaMo or Gramadán. This matches our plan to deploy a single ONNX model in Rust – feeding in additional suggestions would complicate the interface and require marrying the rule-based engine with the neural model. Moreover, integration can be tricky: experience from other languages (e.g. Norwegian) shows that combining a Constraint Grammar analyzer with a neural tagger yields good results only when their tokenization and analysis candidates align well, and integration issues can arise in many cases[\[6\]](https://openreview.net/forum?id=TqEvrDbInx#:~:text=Abstract%3A%20In%20this%20paper%2C%20we,two%20systems%20in%20many%20cases). Given these challenges, a **standalone neural model** is preferred for now. It will be trained purely on the UD treebank, learning likely tags from data. This approach should be sufficient because the UD training annotations were originally derived from a finite-state analyzer and then corrected[\[2\]](https://ceur-ws.org/Vol-1779/09lynn.pdf#:~:text=in%02ventory%20of%20the%20UD%20Project,specific%20labels)[\[1\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Includes%20text%3A%20yes%20Lemmas%3A%20manual,SA%203.0), meaning the model is indirectly learning from both linguistic rules and context-based disambiguation present in the treebank.

**Possibility of Biasing with Analyzer Outputs:** If in the future we find certain rare forms or ambiguous cases that the model struggles with, we could consider a hybrid approach – e.g. providing the model with the rule-based engine’s top candidate tags as an input feature. However, implementing this would require additional engineering (to encode candidate tags for the model) and careful training to make use of them. Since **BuNaMo and Gramadán are separate Rust APIs** and not part of the learned model, the simplest pipeline is to run the ONNX model first to get a full morphological analysis of each token, and then optionally use the rule-based engines afterward for **post-processing or validation** (for example, to flag any inconsistencies or enrich the analysis with additional linguistic info). This keeps the model’s design clean and **minimizes external dependencies**. In summary, the model will be trained and used **without access to analyzer suggestions** at inference – it treats the input as plain text. This is a sound design given our goal of a lightweight, deployable tool. It also future-proofs the pipeline: we can always augment or cross-check the model’s output with BuNaMo/Gramadán results outside the model if needed, without entangling their logic into the ONNX model.  
\- *Gap Note:* Not using the rule-based suggestions means the model must rely solely on what it learned. In practice this should be fine for most tokens (the UD training covers common morphology), but extremely rare or novel word forms might occasionally be mis-tagged. If this becomes an issue, a future enhancement could feed the analyzer’s output into the model or use it as a fallback for unknown words. This is an enhancement path, not a requirement for the initial system.

## **Model Architecture and ONNX I/O Design**

With the scope and inputs settled, we can outline the model architecture and how it will function as an ONNX component in Rust.

* **Encoder – Contextual Representations:** We will use a neural sequence model to encode each token in context. A bidirectional LSTM is a strong choice here for simplicity and performance on limited data. BiLSTMs have been the workhorse of UD pipelines (e.g. UDPipe) for morphological tagging[\[7\]](https://aclanthology.org/L16-1680.pdf#:~:text=,It%20is%20distributed), and they convert readily to ONNX. Each token will be represented by an embedding that captures its form; given Irish’s complex morphology and rich orthography, we should include character-level representations so the model can generalize to unseen words. For example, Stanza’s Irish model leverages a character-level language model to improve morphological accuracy[\[8\]](https://stanfordnlp.github.io/stanza/performance.html#:~:text=All%20models%20are%20trained%20and,possible%2C%20but%20not%20with%20transformers). We can achieve similar benefits by using a CNN or LSTM over characters for each token and concatenating that with a word embedding. This way, the model can infer, say, that a word ending in “-anna” is likely a plural noun, or that a prefixed lenited form “bhialann” corresponds to the lemma “bialann” (restaurant). If available, incorporating pre-trained Irish word embeddings (e.g. FastText vectors for Irish) could further boost performance by providing semantic cues, but this is optional. The encoder will produce a contextual vector for each token (e.g. LSTM hidden state), encoding both the word’s character makeup and its surrounding context.

* **Tagging Output – POS+FEATS Classifier:** On top of the encoder, we’ll have a classifier that predicts the combined morphological tag for each token. We can define each unique combination of UPOS and FEATS that occurs in the training data as a class (for Irish UD there are many possible feature combos, but the set is finite given the corpus). The classifier (e.g. a softmax layer) will output the highest-probability tag class for each token. This yields the UD POS tag plus all applicable features (Gender, Number, Case, etc.) in one go. Using combined “all-tags” classes is straightforward and ensures a coherent set of features per token[\[9\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Word%20of%20caution%20for%20anyone,8). (Alternatively, one could predict UPOS and each feature category with separate outputs, but that gets complex and the features are not independent.) The model thus directly gives the token’s full morphosyntactic label in the UD format. These predicted labels can be mapped back to human-readable strings or kept as feature structures for downstream use. **Accuracy Expectation:** With this design and the UD training data, we can expect high accuracy for POS and features – likely in the mid-90s% range for known-context tokens, based on prior results[\[3\]](https://stanfordnlp.github.io/stanza/performance.html#:~:text=IrishIDTga%2099,16). The model will learn fine distinctions (e.g. distinguishing verb moods or noun cases) from the annotated data. We should be mindful that the UD corpus had some automatically predicted features (especially in older versions)[\[9\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Word%20of%20caution%20for%20anyone,8), but by UD v2.8 most of those were reviewed/corrected[\[10\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Removing%20inconsistencies%3A)[\[1\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Includes%20text%3A%20yes%20Lemmas%3A%20manual,SA%203.0), so training on the latest data should provide reliable ground truth.

* **Lemma Output – Character Sequence Decoder:** To generate lemmas, a possible approach is adding a small sequence-to-sequence decoder for each token’s characters. One efficient design is to use the encoded token representation (from the encoder) as context and feed the character sequence of the word into an LSTM or transformer decoder that outputs the character sequence of the lemma. However, for simplicity (especially in ONNX), we might implement lemma prediction as a separate component: for example, a unidirectional character LSTM that reads the word form and outputs the lemma characters, conditioned on the encoder’s context vector. Another approach is to train a classification model that selects the correct lemma from a candidates list, but this wouldn’t generalize to unseen words. The character-level decoder is more flexible – it can learn rules like “if a verb ends in ‘íonn’, the lemma ends in ‘igh’” or handling eclipsis by dropping the prefix letter (e.g. "bhfaiteadh" \-\> "faiteadh"). We will use the UD treebank’s word–lemma pairs to supervise this. **Integration:** The lemma decoder can be trained jointly with the tagger (multi-task learning) so that the encoder representations are shared. At inference, the ONNX model could output the lemma as a sequence of characters for each token. In practice, ONNX might output a fixed-length array of character IDs per token (padded), or we encode the lemma in a single string tensor if supported. We need to design the I/O such that the Rust side can reconstruct the actual lemma strings from the model output. One simple solution is to have the model output the lemma character-by-character for the longest word length (e.g. max 20 chars), plus an end-of-sequence symbol – the Rust code can trim and join these into the final lemma. This adds a bit of complexity to the model output, but it’s achievable and keeps lemmatization inside our pipeline. If implementation proves tricky, an alternative is to train a standalone lemmatizer model (possibly rule-augmented) and call it separately, but initially aiming for a single ONNX model that does both tagging and lemmatization is efficient.

* **(Optional) Future Extension – Dependency Parsing:** If down the line we decide to incorporate dependency head and relation predictions, the model architecture would need an additional parsing head. Typically, this could be a biaffine attention mechanism producing an $n \\times n$ score matrix for head selection, plus a classifier for relation labels[\[7\]](https://aclanthology.org/L16-1680.pdf#:~:text=,It%20is%20distributed). That parsing head would use the same encoder outputs. For now, we will **not include this**, but the model is designed such that we could extend it later. Notably, our tagging model will already output rich features; a future parser could take these predicted features as inputs (as is common in UD parsing) to improve accuracy[\[11\]](https://ceur-ws.org/Vol-1779/09lynn.pdf#:~:text=reports%20on%20the%20addition%20of,specific%20labels)[\[12\]](https://ceur-ws.org/Vol-1779/09lynn.pdf#:~:text=introduced%20specifically%20for%20the%20Irish,and%20results%20can%20be%20further). For MWE identification, having a full parse with fixed/flat relations would be the solution. Until then, any urgent MWE handling could be done via a simple post-process (e.g. a lexicon of fixed expressions or a regex to join known multi-word tokens) if absolutely required, but again, this is beyond the current model’s scope.

* **Export to ONNX and Rust Integration:** Once trained (likely in PyTorch or TensorFlow), the model will be exported to ONNX format. Both LSTM-based models and transformer-based models are well-supported in ONNX, so we shouldn’t face obstacles in conversion. The ONNX model will have a clear interface:

* **Input:** an integer sequence for the tokenized sentence. Typically this would be word indices (if we have a fixed vocabulary). If using character embeddings internally, those are handled inside the model – from the outside we just pass token IDs or even raw text tokens. Given we want a self-contained model, we might include the embedding lookup in the ONNX graph (so the Rust code doesn’t need to do embedding). Alternatively, we could feed one-hot or vector embeddings from Rust, but that’s unnecessary – easier to let the model handle it given a token ID sequence.

* **Output:** the model can have two outputs: (1) an array of predicted tag IDs (one per token position), and (2) the predicted lemma sequences. For tags, we’ll have a mapping from the ID to the UD tag string (e.g. an ID might map to "NOUN|Gender=Masc|Number=Plur"). The Rust code can apply this mapping to produce human-readable tags or feed them into other components. For lemmas, as discussed, the output might be a 2D array of shape \[max\_tokens, max\_lemma\_length\] with character codes. The Rust integration can translate these codes back to characters and trim at end-of-sequence markers to form the lemma string. This design yields a **simple API** in Rust: you call run(model, sentence\_tokens) and get back a list of (tag, lemma) for each token. The ONNX Runtime in Rust can handle the heavy lifting of the neural network, and our code just does light post-processing on the outputs to map IDs to meaningful labels.

**Note:** We assume one sentence at a time for simplicity; if needed, the pipeline can batch sentences, but since inference is fast and our focus is accuracy, processing sequentially is fine. The treebank-based vocabulary and character handling will ensure that the model can handle most Irish text. For any out-of-vocabulary words, the char component will help the model guess an analysis it hasn’t seen explicitly (leveraging subword patterns). This keeps the model robust without needing the rule-based analyzer in-loop. And because the model is small (relative to huge transformer models) – likely on the order of a few tens of thousands of parameters for the BiLSTM and classifiers, or slightly more if we include char LSTMs – it should be lightweight to load and run in a Rust environment. If we later find accuracy is lacking, we could explore fine-tuning a multilingual transformer (like mBERT or a Gaelic BERT[\[13\]](https://www.sketchengine.eu/irish-universal-dependencies-tagset/#:~:text=Irish%20Universal%20dependencies%20tagset%20,Irish%20corpora%20in%20Sketch%20Engine)) to boost performance, but that would increase model size and complexity. Our proposed architecture is a good balance between **simplicity** and **accuracy**, tailored to the data we have.

## **Summary of Recommended Approach**

1. **Train the model to predict UD POS tags and morphological features for each token, plus lemmas.** Use the Irish UD Treebank as training data (ensuring we use a version where morphological features have been reviewed for consistency[\[10\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Removing%20inconsistencies%3A)[\[1\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Includes%20text%3A%20yes%20Lemmas%3A%20manual,SA%203.0)). Focus on these token-level outputs, which the treebank provides reliably. Do **not** include dependency heads/relations or explicit MWE marking in the initial model – those would require a full parsing component and are not critical for the morphology tools’ current goals.

2. **Design a neural architecture that is compact and exportable:** a BiLSTM encoder with word+character embeddings (to handle Irish orthography) is ideal. This encoder feeds into two heads – one classification head for POS+FEAT tags, and one sequence-generation (or similar) head for lemmas. Multi-task learning will allow the model to leverage shared information (e.g. POS features informing lemma formation). Keep the architecture straightforward to ease ONNX conversion (avoid any non-supported ops).

3. **No dependency on external rule engines during training/inference:** Train the model purely on the annotated data, letting it learn contextual disambiguation. We will not feed BuNaMo or Gramadán outputs as features into the network. At inference, the model will receive only the raw token text (as IDs) and produce its analysis. This ensures the deployed ONNX model is self-contained and easy to call from Rust. (We can, however, use the rule-based tools in pre- or post-processing if needed – for example, to tokenize the input text and to optionally validate model outputs – but they remain outside the model.)

4. **Implement and evaluate:** Train the model and evaluate on the UD Irish dev/test splits to verify performance. We expect high accuracy on POS and core morphological features, and slightly lower but acceptable accuracy on lemmas. For any systematic errors (e.g. the model consistently lemmatizes a certain irregular form incorrectly), consider minor adjustments or add small post-correction rules if absolutely needed. Overall, the goal is to meet or exceed the baseline UD pipeline performance (≈94-96% POS/feats, ≈89% lemma as reported[\[3\]](https://stanfordnlp.github.io/stanza/performance.html#:~:text=IrishIDTga%2099,16)).

5. **Export the model to ONNX and integrate into Rust pipeline:** Once satisfied, export the PyTorch/TensorFlow model to ONNX. Define the model’s input/output schema clearly (e.g. a dynamic axis for sequence length). In the Rust code, use the ONNX Runtime to load this model. The input will be the tokenized sentence (we’ll convert tokens to their index IDs using the same vocab the model was trained on). After inference, map the output tag IDs back to UD labels and assemble the lemma characters into strings. This yields a final structured output for the sentence (each token with its lemma and morphological analysis). The Rust API can then return these or use them in the larger pipeline (e.g. for grammatical error checking, generation, etc.).

6. **Future extensions (if needed):** If later requirements demand syntactic analysis, plan a separate dependency parser module. It could take the output of this morphology model (or even be integrated and jointly fine-tuned). Similarly, if MWE identification becomes crucial (say for idiom detection), we can either utilize the parser’s output (when available) or maintain a small lexicon of known Irish MWEs to post-process the tokens (since many MWEs are fixed phrases that could be listed). These extensions would require additional effort and are noted as potential next steps, but they **do not impede the current roadmap** for the morphological model. The current approach focuses on what’s immediately achievable with available resources and keeps the model focused on its primary role: **accurate morphological analysis from raw Irish text**, delivered via a simple, efficient ONNX component.

Throughout this roadmap, the emphasis is on staying within feasible bounds – leveraging the UD treebank and proven neural architectures – and avoiding “gaps” that would demand large new investments (like crafting complex rule integrations or building a full parser from scratch). By following this plan, we will develop a robust Irish morphology model that plugs into the larger system with ease and can be iteratively improved over time. The end result will be a **plain-text-in → morphological-analysis-out** model, supporting the broader Irish language NLP tools with minimal fuss and maximum consistency with UD standards[\[14\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=A%20Universal%20Dependencies%204910,for%20modern%20Irish)[\[15\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Trees%3A%204910%20Token%20count%3A%2023686,language%20specific%20POS%20tags%3A%2017).

---

[\[1\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Includes%20text%3A%20yes%20Lemmas%3A%20manual,SA%203.0) [\[4\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Note%3A%20,word%20units%20%28fixed) [\[5\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=The%20tokenisation%20of%20the%20Irish,2002) [\[9\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Word%20of%20caution%20for%20anyone,8) [\[10\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Removing%20inconsistencies%3A) [\[14\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=A%20Universal%20Dependencies%204910,for%20modern%20Irish) [\[15\]](https://github.com/UniversalDependencies/UD_Irish-IDT#:~:text=Trees%3A%204910%20Token%20count%3A%2023686,language%20specific%20POS%20tags%3A%2017) GitHub \- UniversalDependencies/UD\_Irish-IDT: Irish data

[https://github.com/UniversalDependencies/UD\_Irish-IDT](https://github.com/UniversalDependencies/UD_Irish-IDT)

[\[2\]](https://ceur-ws.org/Vol-1779/09lynn.pdf#:~:text=in%02ventory%20of%20the%20UD%20Project,specific%20labels) [\[11\]](https://ceur-ws.org/Vol-1779/09lynn.pdf#:~:text=reports%20on%20the%20addition%20of,specific%20labels) [\[12\]](https://ceur-ws.org/Vol-1779/09lynn.pdf#:~:text=introduced%20specifically%20for%20the%20Irish,and%20results%20can%20be%20further) ceur-ws.org

[https://ceur-ws.org/Vol-1779/09lynn.pdf](https://ceur-ws.org/Vol-1779/09lynn.pdf)

[\[3\]](https://stanfordnlp.github.io/stanza/performance.html#:~:text=IrishIDTga%2099,16) [\[8\]](https://stanfordnlp.github.io/stanza/performance.html#:~:text=All%20models%20are%20trained%20and,possible%2C%20but%20not%20with%20transformers) Model Performance \- Stanza

[https://stanfordnlp.github.io/stanza/performance.html](https://stanfordnlp.github.io/stanza/performance.html)

[\[6\]](https://openreview.net/forum?id=TqEvrDbInx#:~:text=Abstract%3A%20In%20this%20paper%2C%20we,two%20systems%20in%20many%20cases) Integrating rules and neural nets for morphological tagging of Norwegian \- Results and challenges | OpenReview

[https://openreview.net/forum?id=TqEvrDbInx](https://openreview.net/forum?id=TqEvrDbInx)

[\[7\]](https://aclanthology.org/L16-1680.pdf#:~:text=,It%20is%20distributed) \[PDF\] UDPipe: Trainable Pipeline for Processing CoNLL-U Files ...

[https://aclanthology.org/L16-1680.pdf](https://aclanthology.org/L16-1680.pdf)

[\[13\]](https://www.sketchengine.eu/irish-universal-dependencies-tagset/#:~:text=Irish%20Universal%20dependencies%20tagset%20,Irish%20corpora%20in%20Sketch%20Engine) Irish Universal dependencies tagset \- Sketch Engine

[https://www.sketchengine.eu/irish-universal-dependencies-tagset/](https://www.sketchengine.eu/irish-universal-dependencies-tagset/)